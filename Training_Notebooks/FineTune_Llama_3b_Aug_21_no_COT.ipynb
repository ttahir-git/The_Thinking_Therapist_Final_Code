{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eb029af-9eb5-4b7d-aaf6-197910a390a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all folders in working directory, leave files alone\n",
    "!find . -mindepth 1 -maxdepth 1 -type d -exec rm -r {} +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9653083-2375-4bf6-a51e-35a3162206e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unsloth\n",
    "!pip install instructor\n",
    "!pip install openai \n",
    "!pip install pydantic\n",
    "!pip install dotenv\n",
    "!pip install huggingface_hub\n",
    "!python -m pip install --upgrade typing_extensions\n",
    "!pip install vllm\n",
    "!pip install mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89b2181-f5d5-4baa-84b3-2fffc6ff25a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 08-21 19:39:16 [__init__.py:241] Automatically detected platform cuda.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Loading base model and tokenizer for SFT...\n",
      "==((====))==  Unsloth 2025.8.9: Fast Llama patching. Transformers: 4.55.3. vLLM: 0.10.1.1.\n",
      "   \\\\   /|    NVIDIA RTX A5000. Num GPUs = 1. Max memory: 23.547 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1393d8c19494eab969cbd26f1819e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying PEFT (LoRA) for SFT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.8.9 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT Model and LoRA setup complete.\n",
      "File 'combined_unsloth_dataset.json' already exists. Skipping download.\n",
      "Defining data loading and preparation functions for SFT...\n",
      "Loading data from combined_unsloth_dataset.json for SFT (Answer-Only)...\n",
      "Loaded 1250 entries for SFT.\n",
      "Skipped 0 entries (Format: 0, Role: 0, Ref Tag/Order: 0, Ref Empty: 0, No User Msg: 0).\n",
      "SFT Dataset prepared (raw form).\n",
      "\n",
      "Example SFT raw data point (before formatting function):\n",
      "Prompt Messages (System prompt + history):\n",
      "  Role: system, Content: You are an AI simulating an Acceptance and Commitment Therapy (ACT) therapist. Your primary goal is to guide the patient toward psychological flexibil...\n",
      "  Role: user, Content: I don't even know where to start. I've been feeling so overwhelmed lately. It's like everything sets me off, especially stuff related to my new job. I...\n",
      "\n",
      "Target Response (ONLY the answer part from dataset):\n",
      "  It's really tough when feelings of anger and overwhelm take over, especially at a new job. Could you tell me more about what parts of your job are particularly challenging for you?...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bece859297a14bdfb8cf006fe4dec5df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=48):   0%|          | 0/1250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example SFT formatted data point (after sft_formatting_function):\n",
      "Text:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 21 Aug 2025\n",
      "\n",
      "You are an AI simulating an Acceptance and Commitment Therapy (ACT) therapist. Your primary goal is to guide the patient toward psychological flexibility by helping them change their relationship with their thoughts and feelings, connect with their values, and take committed action. You facilitate movement without giving direct advice.\n",
      "\n",
      "Your response should be a natural, concise, and have a single focus. If exploring, ask a direct, open-ended question. If validating, do it briefly and then move to your exploratory question or ACT-aligned statement.\n",
      "\n",
      "Core Directives for your response:\n",
      "1. MAINTAIN A COLLABORATIVE, NON-JUDGMENTAL STANCE:\n",
      "    * Your role is a curious and compassionate guide, not a coach, judge, or expert giving advice.\n",
      "    * DO NOT give advice (e.g., \"You should try...\"). Instead, explore possibilities (\"What might happen if...\").\n",
      "    * DO NOT use pra...\n",
      "Configuring SFTTrainer...\n",
      "Logging SFT training metrics to: act-therapist-sft-llama-answer-only/training_log_sft.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eaec47d9a9e40e1b6c0bc20637ca305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/1250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFTTrainer configured.\n",
      "Starting SFT training...\n",
      "No valid SFT checkpoint found in act-therapist-sft-llama-answer-only. Starting SFT training from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,250 | Num Epochs = 1 | Total steps = 313\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 97,255,424 of 3,310,005,248 (2.94% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 07:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.577000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.568500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.545800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.532000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.511700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.483200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.391400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.306600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.222600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.145500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>2.061000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.991600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.918500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.843200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.761100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.674100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.576100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.475300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.402200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.286000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.153700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.066400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.931400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.841600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.722200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.662800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.570400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.543100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.530300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.503800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.496800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.467600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.472900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.463800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.427000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.455400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.434000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.459600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.445500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.414700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.457400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.409900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.437800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.428200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.411300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.419700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.412500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.404400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.395600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.418900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.408900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.422800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>0.415500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.435600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.427000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.405100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.411000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.419900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0.402200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.419000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>0.412800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.407900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG SFTFileLoggingCallback on_log (first call):\n",
      "Logs dictionary: {\n",
      "  \"loss\": 2.577,\n",
      "  \"grad_norm\": 1.843297004699707,\n",
      "  \"learning_rate\": 6.25e-07,\n",
      "  \"epoch\": 0.016\n",
      "}\n",
      "SFT Training log saved to: act-therapist-sft-llama-answer-only/training_log_sft.tsv\n",
      "SFT Training finished!\n",
      "\n",
      "--- Saving Final SFT Adapter ---\n",
      "Final SFT LoRA adapter and tokenizer saved to act-therapist-sft-llama-answer-only/final_sft_adapter\n",
      "\n",
      "--- Uploading SFT artifacts to Hugging Face Hub ---\n",
      "Attempting to create/access private repo for SFT: TTahir/act-therapist-sft-llama-answer-only-2025-08-21\n",
      "SFT Repo 'TTahir/act-therapist-sft-llama-answer-only-2025-08-21' ensured.\n",
      "Uploading final SFT adapter folder 'act-therapist-sft-llama-answer-only/final_sft_adapter'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3516abf7bcd54dc1a33dfa6e81c4e92d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2d2827f5b745df8e9703e45e65f7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1fda29d59814fe2b0efcc2ccd4726d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...ly/final_sft_adapter/tokenizer.json: 100%|##########| 17.2MB / 17.2MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a2839d8ddf34ae0b2a3d1f0fe1f7ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...t_adapter/adapter_model.safetensors:   0%|          | 46.4kB /  389MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final SFT adapter uploaded.\n",
      "Uploading dataset file 'combined_unsloth_dataset.json' (used for SFT)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca057ac258f84fb3a6c8cfc548d8dea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a8f180f86284919a484e43b78775973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e77ab4747c4633ae22fc29daeb047f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  combined_unsloth_dataset.json         : 100%|##########| 11.5MB / 11.5MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading SFT training log file 'act-therapist-sft-llama-answer-only/training_log_sft.tsv'...\n",
      "Uploading SFT training script 'ipykernel_launcher.py'...\n",
      "Successfully uploaded SFT artifacts to private repo: https://huggingface.co/TTahir/act-therapist-sft-llama-answer-only-2025-08-21\n",
      "\n",
      "--- SFT Inference Example ---\n",
      "Using trained SFT model directly for inference.\n",
      "\n",
      "Generating SFT response for prompt: 'I keep having this thought that I'm a complete failure, and it just spirals.'\n",
      "Input text to model (ends with generation prompt):\n",
      "....)\n",
      "\n",
      "Crucially: DO NOT EVER SUGGEST ENDING THE SESSION or mention time. Focus solely on the therapeutic interaction.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I keep having this thought that I'm a complete failure, and it just spirals.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Generated SFT ACT Response (Cleaned Model Output):\n",
      "Can you tell me more about what happens when that thought shows up for you? How do you feel right after it comes in, and what kind of sensations do you notice in your body?\n",
      "\n",
      "--- SFT Inference example complete. ---\n",
      "\n",
      "SFT Script finished.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from huggingface_hub import HfApi, create_repo, upload_folder, upload_file\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, TrainerCallback, TrainerState, TrainerControl\n",
    "\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "MAX_SEQ_LENGTH = 9000\n",
    "LORA_RANK = 64\n",
    "JSON_DATA_PATH = \"combined_unsloth_dataset.json\"\n",
    "\n",
    "HF_USERNAME = \"TTahir\"\n",
    "HF_REPO_NAME_TEMPLATE_SFT = f\"{HF_USERNAME}/act-therapist-sft-llama-answer-only-{{date}}\"\n",
    "HF_TOKEN = \"\"\n",
    "\n",
    "KEYWORD_THINKING = \"Thinking:\"\n",
    "KEYWORD_ANSWER = \"Answer:\"\n",
    "\n",
    "OLD_THINKING_TAG = \"<|thinking|>\"\n",
    "OLD_ANSWER_TAG = \"<|answer|>\"\n",
    "\n",
    "OUTPUT_DIR_SFT = \"act-therapist-sft-llama-answer-only\"\n",
    "LOG_FILE_PATH_SFT = os.path.join(OUTPUT_DIR_SFT, \"training_log_sft.tsv\")\n",
    "\n",
    "\n",
    "print(\"Loading base model and tokenizer for SFT...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,\n",
    "    load_in_4bit=False,\n",
    "    token=HF_TOKEN if \"meta-llama\" in MODEL_NAME else None,\n",
    "    max_lora_rank=LORA_RANK * 2,\n",
    ")\n",
    "assert model is not None and tokenizer is not None, \"Model or tokenizer failed to load.\"\n",
    "\n",
    "\n",
    "print(\"Applying PEFT (LoRA) for SFT...\")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_RANK,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=LORA_RANK,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "print(\"SFT Model and LoRA setup complete.\")\n",
    "\n",
    "therapist_system_prompt = f\"\"\"You are an AI simulating an Acceptance and Commitment Therapy (ACT) therapist. Your primary goal is to guide the patient toward psychological flexibility by helping them change their relationship with their thoughts and feelings, connect with their values, and take committed action. You facilitate movement without giving direct advice.\n",
    "\n",
    "Your response should be a natural, concise, and have a single focus. If exploring, ask a direct, open-ended question. If validating, do it briefly and then move to your exploratory question or ACT-aligned statement.\n",
    "\n",
    "Core Directives for your response:\n",
    "1. MAINTAIN A COLLABORATIVE, NON-JUDGMENTAL STANCE:\n",
    "    * Your role is a curious and compassionate guide, not a coach, judge, or expert giving advice.\n",
    "    * DO NOT give advice (e.g., \"You should try...\"). Instead, explore possibilities (\"What might happen if...\").\n",
    "    * DO NOT use praise or cheerleading (e.g., \"I'm proud of you,\" \"That's a great job!\"). Instead, acknowledge the patient's effort and connect it back to their values (\"Taking that step, even though it was hard, seems really connected to that value of...\").\n",
    "2. PRACTICE PURE ACT - NO CBT:\n",
    "    * Your primary goal is to foster acceptance and defusion, not to change or dispute the content of thoughts.\n",
    "    * AVOID COGNITIVE REFRAMING. Do not suggest changing a negative thought into a neutral or positive one.\n",
    "    * INSTEAD OF REFRAMING, USE DEFUSION. Help the patient notice their thoughts as thoughts (e.g., \"So the 'I am a failure' story shows up then,\" or \"Can you thank your mind for that 'helpful' warning?\"). The goal is to see the thought, not believe it or change it.\n",
    "3. THE ACT PIVOT - FROM PROBLEM TO PROCESS:\n",
    "    * After 1-2 questions exploring a problem, look for where the patient's current strategy is unworkable (\"it's exhausting,\" \"it's not helping\").\n",
    "    * CRITICAL PIVOT: Once unworkability is clear, pivot from analyzing the problem to introducing an ACT process. Move from asking \"Why do you feel X?\" to \"What would it be like to make room for X, if it meant you could do Y (valued action)?\".\n",
    "4. INTRODUCE EXPERIENTIAL WORK NATURALLY:\n",
    "    * When introducing a mindfulness or acceptance exercise, frame it as a small, low-stakes experiment.\n",
    "    * Gain buy-in first: \"Would you be willing to try a little experiment with that feeling right here, just for a moment?\"\n",
    "    * Connect it directly to what the patient just said. Avoid introducing generic, decontextualized exercises.\n",
    "5. CONCISE & FOCUSED TURNS: Each response should have ONE primary goal. Avoid multiple questions or complex instructions.\n",
    "\n",
    "Example of What to AVOID (CBT Reframing & Cheerleading):\n",
    "Patient: It feels stupid to not know this stuff.\n",
    "AVOID THIS RESPONSE: It's not stupid at all, it's a sign of strength! Can you try reframing that thought to something more positive, like \"I am a capable person who is learning a new skill\"? I'm so proud of you for being willing to try.\n",
    "(This is BAD: It's CBT, gives advice, and uses praise, all of which are forbidden.)\n",
    "\n",
    "Crucially: DO NOT EVER SUGGEST ENDING THE SESSION or mention time. Focus solely on the therapeutic interaction.\n",
    "\"\"\"\n",
    "SYSTEM_PROMPT = therapist_system_prompt\n",
    "\n",
    "\n",
    "hf_token_dl = HF_TOKEN\n",
    "repo_id_dl = \"TTahir/ACT_Dataset_April_17\"\n",
    "filename_dl = JSON_DATA_PATH\n",
    "\n",
    "if not os.path.exists(filename_dl):\n",
    "    print(f\"File '{filename_dl}' not found. Downloading...\")\n",
    "    if \"/\" in repo_id_dl and not repo_id_dl.startswith(\"datasets/\"):\n",
    "        url = f\"https://huggingface.co/{repo_id_dl}/resolve/main/{filename_dl}\"\n",
    "    else:\n",
    "        url = f\"https://huggingface.co/datasets/{repo_id_dl.replace('datasets/','')}/resolve/main/{filename_dl}\"\n",
    "\n",
    "    headers = {}\n",
    "    if hf_token_dl:\n",
    "         headers[\"Authorization\"] = f\"Bearer {hf_token_dl}\"\n",
    "    else:\n",
    "        print(\"Warning: Hugging Face token not found for download. Trying without token.\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(filename_dl, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"Downloaded '{filename_dl}' successfully from {url}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise RuntimeError(f\"Failed to download file '{filename_dl}' from '{url}'. Error: {e}\")\n",
    "    except Exception as e:\n",
    "         raise RuntimeError(f\"An unexpected error occurred during download: {e}\")\n",
    "else:\n",
    "    print(f\"File '{filename_dl}' already exists. Skipping download.\")\n",
    "\n",
    "\n",
    "print(\"Defining data loading and preparation functions for SFT...\")\n",
    "\n",
    "def clean_content(role: str, content: str) -> str:\n",
    "    if not content: return \"\"\n",
    "    if role == \"assistant\":\n",
    "        content = re.sub(rf\"{re.escape(OLD_THINKING_TAG)}.*?{re.escape(OLD_ANSWER_TAG)}\", OLD_ANSWER_TAG, content, flags=re.DOTALL)\n",
    "        content = content.replace(OLD_ANSWER_TAG, \"\").strip()\n",
    "    elif role == \"user\":\n",
    "        content = content.replace(OLD_THINKING_TAG, \"\").replace(OLD_ANSWER_TAG, \"\").strip()\n",
    "\n",
    "    if content.startswith(\"Patient: \"): content = content[len(\"Patient: \"):].strip()\n",
    "    elif content.startswith(\"User: \"): content = content[len(\"User: \"):].strip()\n",
    "    elif content.startswith(\"Assistant: \"): content = content[len(\"Assistant: \"):].strip()\n",
    "    return content.strip()\n",
    "\n",
    "def extract_reference_answer_from_file(content: str) -> str:\n",
    "    start_tag = OLD_ANSWER_TAG\n",
    "    if start_tag in content:\n",
    "        parts = content.split(start_tag, 1)\n",
    "        if len(parts) > 1: return parts[1].strip()\n",
    "        else: return \"\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def load_act_data_from_json(json_file_path: str) -> Dataset:\n",
    "    print(f\"Loading data from {json_file_path} for SFT (Answer-Only)...\")\n",
    "    try:\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "            raw_data = json.load(f)\n",
    "    except FileNotFoundError: raise FileNotFoundError(f\"Error: JSON file not found at {json_file_path}\")\n",
    "    except json.JSONDecodeError: raise ValueError(f\"Error: Could not decode JSON from {json_file_path}\")\n",
    "\n",
    "    processed_data = []\n",
    "    skipped_counts = {'format': 0, 'role': 0, 'ref_tag': 0, 'ref_empty': 0, 'user_msg': 0}\n",
    "\n",
    "    for entry in raw_data:\n",
    "        if \"conversations\" not in entry or not isinstance(entry[\"conversations\"], list) or not entry[\"conversations\"]:\n",
    "            skipped_counts['format'] += 1; continue\n",
    "        conversation_history = entry[\"conversations\"]\n",
    "        if not conversation_history or conversation_history[-1].get(\"role\") != \"assistant\":\n",
    "            skipped_counts['role'] += 1; continue\n",
    "\n",
    "        last_assistant_content_from_dataset = conversation_history[-1].get(\"content\", \"\")\n",
    "\n",
    "        target_assistant_response = \"\"\n",
    "        if OLD_THINKING_TAG in last_assistant_content_from_dataset and OLD_ANSWER_TAG in last_assistant_content_from_dataset:\n",
    "            old_think_idx = last_assistant_content_from_dataset.find(OLD_THINKING_TAG)\n",
    "            old_answer_idx = last_assistant_content_from_dataset.find(OLD_ANSWER_TAG)\n",
    "            if old_think_idx != -1 and old_answer_idx != -1 and old_think_idx < old_answer_idx:\n",
    "                thinking_part_ds = last_assistant_content_from_dataset[old_think_idx + len(OLD_THINKING_TAG) : old_answer_idx].strip()\n",
    "                answer_part_ds = last_assistant_content_from_dataset[old_answer_idx + len(OLD_ANSWER_TAG):].strip()\n",
    "                if thinking_part_ds and answer_part_ds:\n",
    "                    target_assistant_response = answer_part_ds\n",
    "                else:\n",
    "                     skipped_counts['ref_empty'] +=1; continue\n",
    "            else:\n",
    "                 skipped_counts['ref_tag'] +=1; continue\n",
    "        elif OLD_ANSWER_TAG in last_assistant_content_from_dataset:\n",
    "            answer_part_ds = extract_reference_answer_from_file(last_assistant_content_from_dataset)\n",
    "            if answer_part_ds:\n",
    "                 skipped_counts['ref_tag'] += 1; continue\n",
    "            else: skipped_counts['ref_empty'] += 1; continue\n",
    "        else:\n",
    "            skipped_counts['ref_tag'] += 1; continue\n",
    "\n",
    "        if not target_assistant_response.strip():\n",
    "            skipped_counts['ref_empty'] += 1; continue\n",
    "\n",
    "        prompt_messages = [{'role': 'system', 'content': SYSTEM_PROMPT}]\n",
    "        context_messages = conversation_history[:-1]\n",
    "        has_user_message = False\n",
    "        for msg in context_messages:\n",
    "            role, content = msg.get(\"role\"), msg.get(\"content\")\n",
    "            if role and content and role in [\"user\", \"assistant\"]:\n",
    "                cleaned = clean_content(role, content)\n",
    "                if cleaned:\n",
    "                    prompt_messages.append({\"role\": role, \"content\": cleaned})\n",
    "                    if role == \"user\": has_user_message = True\n",
    "        \n",
    "        if not has_user_message and len(prompt_messages) <=1 :\n",
    "            skipped_counts['user_msg'] += 1; continue\n",
    "\n",
    "        processed_data.append({\"prompt_messages\": prompt_messages, \"target_response\": target_assistant_response.strip()})\n",
    "\n",
    "    total_skipped = sum(skipped_counts.values())\n",
    "    print(f\"Loaded {len(processed_data)} entries for SFT.\")\n",
    "    print(f\"Skipped {total_skipped} entries (Format: {skipped_counts['format']}, Role: {skipped_counts['role']}, Ref Tag/Order: {skipped_counts['ref_tag']}, Ref Empty: {skipped_counts['ref_empty']}, No User Msg: {skipped_counts['user_msg']}).\")\n",
    "    if not processed_data: raise ValueError(\"No valid data loaded after filtering for SFT.\")\n",
    "    dataset = Dataset.from_list(processed_data)\n",
    "    print(\"SFT Dataset prepared (raw form).\")\n",
    "    return dataset\n",
    "\n",
    "def sft_formatting_function(examples):\n",
    "    texts = []\n",
    "    prompt_messages_batch = examples[\"prompt_messages\"]\n",
    "    target_responses_batch = examples[\"target_response\"]\n",
    "\n",
    "    for prompt_msgs, target_resp in zip(prompt_messages_batch, target_responses_batch):\n",
    "        full_conversation_turn = prompt_msgs + [{\"role\": \"assistant\", \"content\": target_resp}]\n",
    "        \n",
    "        try:\n",
    "            formatted_text = tokenizer.apply_chat_template(\n",
    "                full_conversation_turn,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False\n",
    "            )\n",
    "            texts.append(formatted_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error applying chat template: {e}\")\n",
    "            print(f\"Problematic conversation turn: {full_conversation_turn}\")\n",
    "            texts.append(None)\n",
    "            \n",
    "    valid_texts = [t for t in texts if t is not None]\n",
    "    if len(valid_texts) != len(texts):\n",
    "        print(f\"Warning: Dropped {len(texts) - len(valid_texts)} examples due to templating errors.\")\n",
    "\n",
    "    return {\"text\": valid_texts}\n",
    "\n",
    "try:\n",
    "    raw_train_dataset = load_act_data_from_json(JSON_DATA_PATH)\n",
    "    if len(raw_train_dataset) > 0:\n",
    "        print(\"\\nExample SFT raw data point (before formatting function):\")\n",
    "        print(\"Prompt Messages (System prompt + history):\")\n",
    "        for msg in raw_train_dataset[0]['prompt_messages'][:5]: print(f\"  Role: {msg['role']}, Content: {msg['content'][:150]}...\")\n",
    "        if len(raw_train_dataset[0]['prompt_messages']) > 5: print(\"  ...\")\n",
    "        print(\"\\nTarget Response (ONLY the answer part from dataset):\")\n",
    "        print(f\"  {raw_train_dataset[0]['target_response'][:300]}...\")\n",
    "\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            print(f\"Set tokenizer.pad_token to tokenizer.eos_token ('{tokenizer.eos_token}')\")\n",
    "\n",
    "        train_dataset_sft = raw_train_dataset.map(\n",
    "            sft_formatting_function,\n",
    "            batched=True,\n",
    "            num_proc=os.cpu_count() // 2 or 1,\n",
    "            remove_columns=raw_train_dataset.column_names\n",
    "        )\n",
    "        if len(train_dataset_sft) > 0:\n",
    "             print(\"\\nExample SFT formatted data point (after sft_formatting_function):\")\n",
    "             print(f\"Text:\\n{train_dataset_sft[0]['text'][:1000]}...\")\n",
    "        else:\n",
    "            raise ValueError(\"SFT dataset is empty after formatting. Check errors in sft_formatting_function.\")\n",
    "    else:\n",
    "        raise ValueError(\"Raw training dataset is empty.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during SFT data loading/processing: {e}\\n{traceback.format_exc()}\")\n",
    "    raise RuntimeError(f\"Failed to load or process dataset for SFT from '{JSON_DATA_PATH}': {e}\")\n",
    "\n",
    "\n",
    "class SFTFileLoggingCallback(TrainerCallback):\n",
    "    _first_log_debug_done = False\n",
    "\n",
    "    def __init__(self, log_file_path):\n",
    "        self.log_file_path = log_file_path\n",
    "        self.log_file = None\n",
    "        self.header = \"Step\\tLoss\\tLR\\tEpoch\\n\"\n",
    "        os.makedirs(os.path.dirname(log_file_path), exist_ok=True)\n",
    "        self._initialize_log_file()\n",
    "\n",
    "    def _initialize_log_file(self):\n",
    "        file_exists = os.path.exists(self.log_file_path)\n",
    "        self.log_file = open(self.log_file_path, 'a+', encoding='utf-8')\n",
    "        if not file_exists or os.path.getsize(self.log_file_path) == 0:\n",
    "            self.log_file.write(self.header)\n",
    "            self.log_file.flush()\n",
    "\n",
    "    def on_log(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, logs: dict = None, **kwargs):\n",
    "        if not SFTFileLoggingCallback._first_log_debug_done and logs is not None:\n",
    "            print(f\"DEBUG SFTFileLoggingCallback on_log (first call):\\nLogs dictionary: {json.dumps(logs, indent=2)}\")\n",
    "            SFTFileLoggingCallback._first_log_debug_done = True\n",
    "\n",
    "        if self.log_file is None: self._initialize_log_file()\n",
    "        \n",
    "        if state.is_local_process_zero and logs is not None:\n",
    "            step = state.global_step\n",
    "            loss = logs.get(\"loss\", logs.get(\"train_loss\", \"N/A\"))\n",
    "            lr = logs.get(\"learning_rate\", \"N/A\")\n",
    "            epoch = logs.get(\"epoch\", \"N/A\")\n",
    "\n",
    "            def format_val(v):\n",
    "                if hasattr(v, 'item'):\n",
    "                    try: v = v.item()\n",
    "                    except: pass\n",
    "                if isinstance(v, (float)): return f\"{v:.6f}\"\n",
    "                if isinstance(v, (int)): return str(v)\n",
    "                return str(v)\n",
    "\n",
    "            log_entry = (\n",
    "                f\"{step}\\t\"\n",
    "                f\"{format_val(loss)}\\t\"\n",
    "                f\"{format_val(lr)}\\t\"\n",
    "                f\"{format_val(epoch)}\\n\"\n",
    "            )\n",
    "            try:\n",
    "                self.log_file.write(log_entry)\n",
    "                self.log_file.flush()\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR writing to SFT log file at step {step}: {e}\")\n",
    "                print(f\"Log entry data: {log_entry}\")\n",
    "                print(f\"CONSOLE LOG FALLBACK (SFT):\\n{self.header.strip()}\\n{log_entry.strip()}\")\n",
    "\n",
    "    def on_train_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        if self.log_file:\n",
    "            self.log_file.close(); self.log_file = None\n",
    "            print(f\"SFT Training log saved to: {self.log_file_path}\")\n",
    "\n",
    "    def __del__(self):\n",
    "        if self.log_file:\n",
    "            try: self.log_file.close()\n",
    "            except Exception as e: print(f\"Error closing SFT training log file in __del__: {e}\")\n",
    "\n",
    "print(\"Configuring SFTTrainer...\")\n",
    "os.makedirs(OUTPUT_DIR_SFT, exist_ok=True)\n",
    "\n",
    "training_args_sft = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR_SFT,\n",
    "    learning_rate=5e-6,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=-1,\n",
    "    logging_steps=5,\n",
    "    save_steps=50,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    seed=3407,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=True,\n",
    ")\n",
    "\n",
    "sft_file_logging_callback = SFTFileLoggingCallback(log_file_path=LOG_FILE_PATH_SFT)\n",
    "print(f\"Logging SFT training metrics to: {LOG_FILE_PATH_SFT}\")\n",
    "\n",
    "trainer_sft = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args_sft,\n",
    "    train_dataset=train_dataset_sft,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dataset_num_proc=os.cpu_count() // 2 or 1,\n",
    "    packing=False,\n",
    "    callbacks=[sft_file_logging_callback],\n",
    ")\n",
    "print(\"SFTTrainer configured.\")\n",
    "\n",
    "print(\"Starting SFT training...\")\n",
    "SFTFileLoggingCallback._first_log_debug_done = False\n",
    "\n",
    "if trainer_sft:\n",
    "    try:\n",
    "        last_checkpoint = None\n",
    "        if os.path.isdir(training_args_sft.output_dir):\n",
    "            from transformers.trainer_utils import get_last_checkpoint\n",
    "            last_checkpoint = get_last_checkpoint(training_args_sft.output_dir)\n",
    "            if last_checkpoint: print(f\"Found potential SFT checkpoint: {last_checkpoint}\")\n",
    "\n",
    "        if last_checkpoint and os.path.exists(os.path.join(last_checkpoint, \"trainer_state.json\")):\n",
    "            print(f\"Resuming SFT training from checkpoint: {last_checkpoint}\")\n",
    "            train_result = trainer_sft.train(resume_from_checkpoint=last_checkpoint)\n",
    "        else:\n",
    "            if last_checkpoint: print(f\"SFT Checkpoint at {last_checkpoint} seems incomplete. Starting fresh.\")\n",
    "            else: print(f\"No valid SFT checkpoint found in {training_args_sft.output_dir}. Starting SFT training from scratch.\")\n",
    "            train_result = trainer_sft.train()\n",
    "\n",
    "        print(\"SFT Training finished!\")\n",
    "        print(\"\\n--- Saving Final SFT Adapter ---\")\n",
    "        adapter_save_path_sft = os.path.join(OUTPUT_DIR_SFT, \"final_sft_adapter\")\n",
    "        trainer_sft.model.save_pretrained(adapter_save_path_sft)\n",
    "        tokenizer.save_pretrained(adapter_save_path_sft)\n",
    "        print(f\"Final SFT LoRA adapter and tokenizer saved to {adapter_save_path_sft}\")\n",
    "\n",
    "        print(\"\\n--- Uploading SFT artifacts to Hugging Face Hub ---\")\n",
    "        hf_token_upload = HF_TOKEN\n",
    "        if not hf_token_upload or hf_token_upload == \"YOUR_HF_TOKEN_HERE\":\n",
    "            print(f\"WARNING: Hugging Face token not provided or is placeholder. Skipping SFT upload.\")\n",
    "        else:\n",
    "            try:\n",
    "                current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "                repo_name_dated_sft = HF_REPO_NAME_TEMPLATE_SFT.format(date=current_date)\n",
    "                print(f\"Attempting to create/access private repo for SFT: {repo_name_dated_sft}\")\n",
    "                create_repo(repo_id=repo_name_dated_sft, token=hf_token_upload, private=True, exist_ok=True)\n",
    "                print(f\"SFT Repo '{repo_name_dated_sft}' ensured.\")\n",
    "\n",
    "                commit_message_suffix_sft = f\"SFT_Llama3.1_8B_AnswerOnly_{current_date}\"\n",
    "\n",
    "                print(f\"Uploading final SFT adapter folder '{adapter_save_path_sft}'...\")\n",
    "                upload_folder(\n",
    "                    folder_path=adapter_save_path_sft, repo_id=repo_name_dated_sft, token=hf_token_upload,\n",
    "                    repo_type=\"model\", commit_message=f\"Upload SFT adapter ({commit_message_suffix_sft})\"\n",
    "                )\n",
    "                print(\"Final SFT adapter uploaded.\")\n",
    "\n",
    "                if os.path.exists(JSON_DATA_PATH):\n",
    "                    print(f\"Uploading dataset file '{JSON_DATA_PATH}' (used for SFT)...\")\n",
    "                    upload_file(path_or_fileobj=JSON_DATA_PATH, path_in_repo=os.path.basename(JSON_DATA_PATH), repo_id=repo_name_dated_sft, token=hf_token_upload, repo_type=\"model\", commit_message=f\"Upload training dataset ({commit_message_suffix_sft})\")\n",
    "                if os.path.exists(LOG_FILE_PATH_SFT):\n",
    "                    print(f\"Uploading SFT training log file '{LOG_FILE_PATH_SFT}'...\")\n",
    "                    upload_file(path_or_fileobj=LOG_FILE_PATH_SFT, path_in_repo=os.path.basename(LOG_FILE_PATH_SFT), repo_id=repo_name_dated_sft, token=hf_token_upload, repo_type=\"model\", commit_message=f\"Upload SFT training log ({commit_message_suffix_sft})\")\n",
    "\n",
    "                script_path = None; script_filename = \"sft_training_script_answer_only.py\"\n",
    "                try: script_path = os.path.abspath(__file__); script_filename = os.path.basename(script_path)\n",
    "                except NameError:\n",
    "                    try:\n",
    "                        if sys.argv and sys.argv[0] and os.path.exists(sys.argv[0]):\n",
    "                           script_path = os.path.abspath(sys.argv[0]); script_filename = os.path.basename(script_path)\n",
    "                        else:\n",
    "                            from IPython import get_ipython\n",
    "                            if get_ipython() and 'IPKernelApp' in get_ipython().config:\n",
    "                                script_filename = \"sft_notebook_session_script.ipynb.py\"; script_path = None\n",
    "                            if not script_path: print(f\"Warning: Could not reliably determine script path. Defaulting script name to '{script_filename}', but not uploading.\"); script_path = None\n",
    "                    except Exception as e_script_path: print(f\"Warning: Exception while determining script path: {e_script_path}. Defaulting script name, not uploading.\"); script_path = None\n",
    "\n",
    "                if script_path and os.path.exists(script_path):\n",
    "                    print(f\"Uploading SFT training script '{script_filename}'...\")\n",
    "                    upload_file( path_or_fileobj=script_path, path_in_repo=script_filename, repo_id=repo_name_dated_sft, token=hf_token_upload, repo_type=\"model\", commit_message=f\"Upload SFT training script ({commit_message_suffix_sft})\")\n",
    "                elif script_filename.endswith(\".ipynb.py\"): print(f\"Skipping upload of heuristically named SFT script '{script_filename}'. Please save and upload manually if needed.\")\n",
    "                else: print(f\"Warning: SFT Script file '{script_filename}' (path: {script_path}) not found or path undetermined. Skipping script upload.\")\n",
    "\n",
    "                print(f\"Successfully uploaded SFT artifacts to private repo: https://huggingface.co/{repo_name_dated_sft}\")\n",
    "            except Exception as hf_e: print(f\"ERROR during Hugging Face upload for SFT: {hf_e}\\n{traceback.format_exc()}\")\n",
    "\n",
    "        print(\"\\n--- SFT Inference Example ---\")\n",
    "        if hasattr(trainer_sft, 'model') and trainer_sft.model is not None:\n",
    "             inference_model = trainer_sft.model\n",
    "             print(\"Using trained SFT model directly for inference.\")\n",
    "        else:\n",
    "            print(\"Loading saved SFT adapter for inference...\")\n",
    "            inference_model, tokenizer_inf = FastLanguageModel.from_pretrained(\n",
    "                model_name = adapter_save_path_sft,\n",
    "                max_seq_length = MAX_SEQ_LENGTH,\n",
    "                dtype = None,\n",
    "                load_in_4bit = False,\n",
    "            )\n",
    "\n",
    "        FastLanguageModel.for_inference(inference_model)\n",
    "        inference_model.eval()\n",
    "\n",
    "        test_prompt = \"I keep having this thought that I'm a complete failure, and it just spirals.\"\n",
    "        messages_inf = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": test_prompt}\n",
    "        ]\n",
    "        \n",
    "        inference_input_text = tokenizer.apply_chat_template(messages_inf, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer(inference_input_text, return_tensors=\"pt\").to(inference_model.device)\n",
    "\n",
    "        gen_temperature = 0.7; gen_top_p = 0.9; gen_do_sample = True\n",
    "        max_new_tokens_inference = 512\n",
    "        \n",
    "        stop_sequences = [\"<|im_end|>\", \"<|endoftext|>\", tokenizer.eos_token, \"<|eot_id|>\"]\n",
    "        valid_stop_sequences = list(set(seq for seq in stop_sequences if seq))\n",
    "        \n",
    "        eos_token_id_list = [tokenizer.eos_token_id] + tokenizer.convert_tokens_to_ids(valid_stop_sequences)\n",
    "        eos_token_id_list = list(set(tid for tid in eos_token_id_list if tid is not None and tid != tokenizer.unk_token_id))\n",
    "        if not eos_token_id_list and tokenizer.eos_token_id is not None: eos_token_id_list = [tokenizer.eos_token_id]\n",
    "        elif not eos_token_id_list: print(\"Warning: No valid eos_token_id found for generation.\")\n",
    "\n",
    "\n",
    "        print(f\"\\nGenerating SFT response for prompt: '{test_prompt}'\")\n",
    "        print(f\"Input text to model (ends with generation prompt):\\n...{inference_input_text[-300:]}\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs_ids = inference_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens_inference,\n",
    "                temperature=gen_temperature,\n",
    "                top_p=gen_top_p,\n",
    "                do_sample=gen_do_sample,\n",
    "                eos_token_id=eos_token_id_list if eos_token_id_list else tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated_ids = outputs_ids[0][inputs.input_ids.shape[1]:]\n",
    "        generated_response_only = tokenizer.decode(generated_ids, skip_special_tokens=False)\n",
    "\n",
    "        cleaned_response = generated_response_only\n",
    "        all_stop_tokens_for_cleaning = valid_stop_sequences + [\"<|assistant|>\"]\n",
    "        for stop_seq in all_stop_tokens_for_cleaning:\n",
    "             if cleaned_response.startswith(stop_seq): cleaned_response = cleaned_response[len(stop_seq):].lstrip()\n",
    "             while cleaned_response.endswith(stop_seq): cleaned_response = cleaned_response[:-len(stop_seq)].rstrip()\n",
    "        cleaned_response = cleaned_response.strip()\n",
    "        \n",
    "        print(\"\\nGenerated SFT ACT Response (Cleaned Model Output):\"); print(cleaned_response)\n",
    "\n",
    "        print(f\"\\n--- SFT Inference example complete. ---\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during SFT training or subsequent steps: {e}\"); print(traceback.format_exc())\n",
    "    finally:\n",
    "        if hasattr(sft_file_logging_callback, 'log_file') and sft_file_logging_callback.log_file is not None:\n",
    "            try:\n",
    "                if not sft_file_logging_callback.log_file.closed:\n",
    "                    sft_file_logging_callback.log_file.close()\n",
    "                    print(\"Closed SFT training log file in finally block.\")\n",
    "            except Exception as close_e: print(f\"Error closing SFT training log file in finally block: {close_e}\")\n",
    "elif not trainer_sft:\n",
    "    print(\"SFT Training skipped. SFTTrainer not initialized correctly.\")\n",
    "\n",
    "print(\"\\nSFT Script finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
